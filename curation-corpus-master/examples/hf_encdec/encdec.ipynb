{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Leveraging Huggingface's Encoder Decoder Framework for Abstractive Summarisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm seq2seq_trainer.py\n",
    "# !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel, BertTokenizerFast, TrainingArguments\n",
    "\n",
    "from seq2seq_trainer import Seq2SeqTrainer"
   ]
  },
  {
   "source": [
    "## Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenisation_batch_size = 1024\n",
    "encoder_max_length = 512\n",
    "decoder_max_length = 128\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/filtered_sources_with_bodies.csv')[-40000:]\n",
    "df = df.fillna(\"\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "val_data = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3ff5d215b3049f889a7d1a624ee3adb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d860e69effb745c7aed3ca293fdbcbf9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "439e5509bfe04446b5e5e03f35d3d87d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    inputs = tokenizer(batch[\"article_content\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=tokenisation_batch_size, \n",
    "    remove_columns=[\"title\", \"article_content\", \"summary\", \"url\", \"date\"]\n",
    ")\n",
    "\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=tokenisation_batch_size, \n",
    "    remove_columns=[\"title\", \"article_content\", \"summary\", \"url\", \"date\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "train_data.save_to_disk(\"data/encoded_curation_corpus_large_train_data\")\n",
    "val_data.save_to_disk(\"data/encoded_curation_corpus_large_val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = datasets.load_from_disk(\"data/encoded_curation_corpus_large_train_data\")\n",
    "# val_data = datasets.load_from_disk(\"data/encoded_curation_corpus_large_val_data\")"
   ]
  },
  {
   "source": [
    "## Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We could create our bert2bert model as shown in the commented out line below. But since Patrick von Platen has already trained a model on the CNN/DailyMail dataset we might as well do some transfer learning. So we'll use that model's weights as our starting point."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n",
    "bert2bert = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.eos_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 142\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    label_smoothing: Optional[float] = field(\n",
    "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
    "    )\n",
    "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
    "    predict_with_generate: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
    "    )\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
    "    encoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    decoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
    "    attention_dropout: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    lr_scheduler: Optional[str] = field(\n",
    "        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "source": [
    "On a single V100 GPU this takes about 1 hour per epoch. I've only run it for about 10 minutes because I'm impatient but you could run it for as long as you like!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    evaluate_during_training=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    eval_steps=8000,\n",
    "    warmup_steps=2000,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=True, \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "source": [
    "## Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa7e3fe3aa3b45f190fd90501f75eeda"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore(precision=0.18080042450138092, recall=0.14232230690271044, fmeasure=0.1561764867738723)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"./models/checkpoint-3000\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "test_data = Dataset.from_pandas(test_df)\n",
    "test_data = test_data.select(range(64))\n",
    "\n",
    "def generate_summary(batch):\n",
    "    inputs = tokenizer(batch[\"article_content\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    batch[\"pred\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=16, remove_columns=[\"article_content\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"summary\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\nUber is the biggest ride-sharing service in the world. It’s already present in countless countries and continues to grow. It does have to deal with stringent local regulations and other issues in tough markets which often lead to the company have to take a swift decision. It took one such decision in Abu Dhabi – the capital of the United Arab Emirates – by temporarily suspending its service after some drivers were reportedly detained.\\n\\nAdvertising\\n\\nThe issue reportedly stems from a dispute between regulators and regional rival Careem. Uber is said to have suspended its service in the city after some drivers working for Careem were detained.\\n\\nIt’s believed that as many as eight Careem drivers have been detained, while local media reports suggest that up to 50 drivers working for both Uber and Careem have been arrested so far.\\n\\n“Uber made the decision to temporarily suspend services due to some unforeseen circumstances. Our goal is to have operations up and running as soon as possible,” a spokesperson for the company said today without highlighting the steps that Uber is taking to address this issue. Uber did clarify that none of the drivers were working for it when they were detained.\\n\\nReports suggest that one of the reason for the dispute between the parties is that regulators in Abu Dhabi have been calling on ride-sharing services like Uber and Careem to raise their fares and also to comply with licensing requirements which regulate the number of drivers available for hire. The capital’s taxi regulator has not yet commented on the matter.\\n\\nFiled in . Read more about Uae and Uber. Source: ft'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "test_data[0]['article_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Ride-hailing services Uber and Careem have \"temporarily suspended\" operations in the United Arab Emirates\\' capital Abu Dhabi, following the arrest of between eight and 50 drivers, according to unnamed sources. It is believed the drivers were detained because of \"violations of regulations,\" but it is unclear which regulations are involved.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "label_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'uber is the biggest ride - sharing service in the world. it took one such decision in abu dhabi, temporarily suspending its service after some drivers were detained. it is believed that as many as eight careem drivers have been detained, while local media reports suggest that up to 50 drivers working for both uber and careem have been arrested. the issue is believed to have stemmed from a dispute between regulators and regional rival careem.'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "pred_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}